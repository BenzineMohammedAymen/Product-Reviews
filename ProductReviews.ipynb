{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenzineMohammedAymen/Product-Reviews/blob/main/ProductReviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6LbosluYAQn",
        "outputId": "4fa8c077-e63a-48e2-ff1d-2fe5034df8a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Downloading amazon-reviews.zip to /content\n",
            " 99% 1.28G/1.29G [00:13<00:00, 47.5MB/s]\n",
            "100% 1.29G/1.29G [00:14<00:00, 99.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install keras\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d kritanjalijain/amazon-reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e8p6fptWfol",
        "outputId": "6aeda1bc-38e5-480d-d434-f489fd61372c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  amazon-reviews.zip\n",
            "  inflating: data/amazon_review_polarity_csv.tgz  \n",
            "  inflating: data/test.csv           \n",
            "  inflating: data/train.csv          \n"
          ]
        }
      ],
      "source": [
        "!unzip amazon-reviews.zip -d data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AP6rl03OFTc",
        "outputId": "8fd204e5-1077-444c-9a02-4d03e7a656bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "importing modules ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "importing modules complete !\n",
            "reading the data ...\n",
            "reading data complete !\n",
            "splitting data ...\n",
            "splitting the data complete !\n",
            "defining functions ...\n",
            "defining functions complete !\n",
            "data processing ...\n",
            "data processing complete !\n",
            "converting to numbers ...\n",
            "converting to numbers complete !\n",
            "train test splitting ...\n",
            "splitting complete !\n",
            "importing modules ...\n",
            "importing modules complete !\n",
            "Creating the model ...\n",
            "model complete !\n",
            "compiling model ...\n",
            "compiling complete !\n",
            "training ...\n",
            "Epoch 1/7\n",
            "3462/3462 [==============================] - 25s 5ms/step - loss: 0.3600 - precision: 0.8408\n",
            "Epoch 2/7\n",
            "3462/3462 [==============================] - 18s 5ms/step - loss: 0.2727 - precision: 0.8898\n",
            "Epoch 3/7\n",
            "3462/3462 [==============================] - 18s 5ms/step - loss: 0.1224 - precision: 0.9584\n",
            "Epoch 4/7\n",
            "3462/3462 [==============================] - 18s 5ms/step - loss: 0.0328 - precision: 0.9884\n",
            "Epoch 5/7\n",
            "3462/3462 [==============================] - 18s 5ms/step - loss: 0.0163 - precision: 0.9940\n",
            "Epoch 6/7\n",
            "3462/3462 [==============================] - 18s 5ms/step - loss: 0.0113 - precision: 0.9959\n",
            "Epoch 7/7\n",
            "3462/3462 [==============================] - 18s 5ms/step - loss: 0.0093 - precision: 0.9963\n",
            "training complete !\n",
            "evaluating ...\n",
            "1731/1731 [==============================] - 5s 3ms/step - loss: 0.9560 - precision: 0.8674\n",
            "evaluating complete !\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model saved !\n",
            "like product\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "prediction [[0.9456015]]\n",
            "[[3.7155583e-05]]\n"
          ]
        }
      ],
      "source": [
        "print(\"importing modules ...\")\n",
        "\n",
        "import pandas as pd # to read data\n",
        "from keras.preprocessing.text import Tokenizer # for the dictionary\n",
        "from sklearn.model_selection import train_test_split # for splitting\n",
        "import pickle # to save files\n",
        "from nltk.stem import PorterStemmer # for stemming\n",
        "from nltk.tokenize import word_tokenize # to help stem\n",
        "from nltk.corpus import stopwords # to help stem\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english')) # to help stem\n",
        "import re # to help stem\n",
        "\n",
        "print(\"importing modules complete !\")\n",
        "\n",
        "print(\"reading the data ...\")\n",
        "path = '/content/drive/MyDrive/train.csv'\n",
        "df = pd.read_csv(path) # reading data\n",
        "\n",
        "print(\"reading data complete !\")\n",
        "\n",
        "print(\"splitting data ...\")\n",
        "x = df.iloc[:(int)(len(df.iloc[:,2])/13),2]\n",
        "y = df.iloc[:(int)(len(df.iloc[:,0])/13),0]\n",
        "del df\n",
        "print(\"splitting the data complete !\")\n",
        "inputLayer = 2250\n",
        "\n",
        "print(\"defining functions ...\")\n",
        "\n",
        "tokenizer = Tokenizer(num_words=inputLayer) # convert words to numbers\n",
        "def creating_dictionary(comments):\n",
        "    global tokenizer\n",
        "    tokenizer.fit_on_texts(comments)\n",
        "    data = tokenizer.texts_to_matrix(comments, mode='tfidf')\n",
        "    return data\n",
        "\n",
        "def convert_to_numbers(comments):\n",
        "    global tokenizer\n",
        "    data = tokenizer.texts_to_matrix(comments, mode='tfidf')\n",
        "    return data\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def data_processing(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text_tokens = word_tokenize(text)\n",
        "    filtered_text = [w for w in text_tokens if not w in stop_words]\n",
        "    stemmed_text = [stemmer.stem(word) for word in filtered_text]\n",
        "    return \" \".join(stemmed_text)\n",
        "\n",
        "print(\"defining functions complete !\")\n",
        "\n",
        "#applying data processing\n",
        "print(\"data processing ...\")\n",
        "#df.iloc[:,2] = df.iloc[:,2].apply(data_processing)\n",
        "x = x.apply(data_processing)\n",
        "\n",
        "print(\"data processing complete !\")\n",
        "\n",
        "y -= 1\n",
        "\n",
        "print(\"converting to numbers ...\")\n",
        "\n",
        "x = creating_dictionary(x)\n",
        "with open('/content/drive/MyDrive/tokenizer.pickle', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "print(\"converting to numbers complete !\")\n",
        "\n",
        "print(\"train test splitting ...\")\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.20,random_state =42)\n",
        "\n",
        "del x\n",
        "del y\n",
        "\n",
        "print(\"splitting complete !\")\n",
        "\n",
        "print(\"importing modules ...\")\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import keras\n",
        "#import numpy as np\n",
        "\n",
        "print(\"importing modules complete !\")\n",
        "\n",
        "print(\"Creating the model ...\")\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(units=inputLayer, activation='relu'))\n",
        "model.add(Dense(units=8, activation='relu'))\n",
        "model.add(Dense(units=8, activation='relu'))\n",
        "model.add(Dense(units=8, activation='relu'))\n",
        "model.add(Dense(units=8, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "print(\"model complete !\")\n",
        "\n",
        "#y_train = y_train.values\n",
        "\n",
        "#x_train_normalized = (x_train - np.min(x_train)) / (np.max(x_train) - np.min(x_train))\n",
        "#x_test_normalized = (x_train - np.min(x_train)) / (np.max(x_train) - np.min(x_train))\n",
        "\n",
        "\n",
        "print(\"compiling model ...\")\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[keras.metrics.Precision()])\n",
        "\n",
        "print(\"compiling complete !\")\n",
        "\n",
        "#training the model\n",
        "print(\"training ...\")\n",
        "history = model.fit(x_train, y_train, batch_size=64, epochs=7)\n",
        "print(\"training complete !\")\n",
        "print(\"evaluating ...\")\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"evaluating complete !\")\n",
        "model.save(\"/content/drive/MyDrive/model.model\")\n",
        "\n",
        "print(\"model saved !\")\n",
        "text = 'i like this product'\n",
        "text2 = 'i hate and dislike this product'\n",
        "text = data_processing(text)\n",
        "text2 = data_processing(text2)\n",
        "\n",
        "test_data = convert_to_numbers([text])\n",
        "test_data2 = convert_to_numbers([text2])\n",
        "print(text)\n",
        "prediction = model.predict(test_data)\n",
        "prediction2 = model.predict(test_data2)\n",
        "print(\"prediction\",prediction)\n",
        "print(prediction2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3x9shXQuqKU",
        "outputId": "ebfa508f-295e-440c-b531-bb1b625aaa1a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 2250)              5064750   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8)                 18008     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 8)                 72        \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,082,983\n",
            "Trainable params: 5,082,983\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "prediction [8.203315e-05]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "prediction [9.042506e-08]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "prediction [0.9412631]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "prediction [0.08350866]\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "#with open('Network.pickle', 'rb') as f:\n",
        "#    neuralNetwork = pickle.load(f)\n",
        "\n",
        "import tensorflow as tf   #machine learning\n",
        "from nltk.stem import PorterStemmer # for stemming\n",
        "from nltk.tokenize import word_tokenize # to help stem\n",
        "from nltk.corpus import stopwords # to help stem\n",
        "from keras.preprocessing.text import Tokenizer # for the dictionary\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from keras.models import load_model\n",
        "stop_words = set(stopwords.words('english'))\n",
        "load_options = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
        "model = load_model('/content/drive/MyDrive/model.model')\n",
        "model.summary()\n",
        "\n",
        "def convert_to_numbers(comments):\n",
        "    with open('/content/drive/MyDrive/tokenizer.pickle', 'rb') as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "    data = tokenizer.texts_to_matrix(comments, mode='tfidf')\n",
        "    return data\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def data_processing(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text_tokens = word_tokenize(text)\n",
        "    filtered_text = [w for w in text_tokens if not w in stop_words]\n",
        "    stemmed_text = [stemmer.stem(word) for word in filtered_text]\n",
        "    return \" \".join(stemmed_text)\n",
        "\n",
        "def test():\n",
        "    again = True\n",
        "    while again:\n",
        "        text = input('write a comment: ')\n",
        "        text = data_processing(text)\n",
        "        test_data = convert_to_numbers([text])\n",
        "        prediction = model.predict(test_data)\n",
        "        print(\"prediction\",prediction[0])\n",
        "\n",
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "IcQlcaaRHvvD",
        "outputId": "85f28f2f-e9f9-49fc-c654-e77fe8c7e3af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dislik product work weak hope store minim price good product\n",
            "product amaz lie\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-83428e54b9f4>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_data2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_numbers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprediction2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_UserObject' object has no attribute 'predict'"
          ]
        }
      ],
      "source": [
        "\n",
        "text = \"i dislike this product . it is not working and weak i hope this store minimize the price to be a good for the product \"\n",
        "text2 =\"this product is an  amazing lie \"\n",
        "text = data_processing(text)\n",
        "text2 = data_processing(text2)\n",
        "print(text)\n",
        "print(text2)\n",
        "test_data = convert_to_numbers([text])\n",
        "test_data2 = convert_to_numbers([text2])\n",
        "\n",
        "prediction = model.predict(test_data)\n",
        "prediction2 = model.predict(test_data2)\n",
        "print(\"prediction\",prediction)\n",
        "print(prediction2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpK4xNDyFCAk",
        "outputId": "22151f1a-55d7-4748-8197-a20c031cd4c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neural network model visualization saved as neural_network_model.png\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Define your neural network model\n",
        "mode = Sequential()\n",
        "mode.add(Dense(8, input_shape=(2250,), activation='relu'))\n",
        "mode.add(Dense(8, activation='relu'))\n",
        "mode.add(Dense(8, activation='relu'))\n",
        "mode.add(Dense(8, activation='relu'))\n",
        "mode.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "mode.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Generate a visualization of the model\n",
        "plot_model(mode, to_file='neural_network_model.png', show_shapes=True)\n",
        "\n",
        "print(\"Neural network model visualization saved as neural_network_model.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4xSxukFS5l_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def binray_cross_entropy(y, p):\n",
        "    e = 1e-5\n",
        "    output = -(y * np.log(p + e) + (1 - y) * np.log(1 - p + e))\n",
        "    normalized_output = output\n",
        "    return normalized_output\n",
        "\n",
        "def binray_cross_entropy_derivative(y, p):\n",
        "    e = 1e-5\n",
        "    return (p - y ) / ((p + e) * (1 - p + e))\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, input_size, output_size, activation):\n",
        "        self.weights = np.random.randn(output_size,input_size)\n",
        "        self.bias = np.zeros((output_size,1))\n",
        "        self.activation = activation\n",
        "        self.inputs = None\n",
        "        self.outputs = None\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.m_w = np.zeros_like(self.weights)\n",
        "        self.v_w = np.zeros_like(self.weights)\n",
        "\n",
        "        self.m_b = np.zeros_like(self.bias)\n",
        "        self.v_b = np.zeros_like(self.bias)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        z = np.dot(self.weights, self.inputs) + self.bias\n",
        "        self.outputs = self.activation(z)\n",
        "        return self.outputs\n",
        "\n",
        "    def backward(self, delta):\n",
        "        activation_derivative = self.activation(self.outputs, derivative=True)\n",
        "        self.delta = delta * activation_derivative\n",
        "        input_delta = np.dot(self.weights.T, self.delta)\n",
        "        return input_delta\n",
        "\n",
        "    def update_weights(self, learning_rate, beta1, beta2, epsilon, t, l1_lambda = 0):\n",
        "        dW = np.dot(self.delta, self.inputs.T)\n",
        "        dB = np.sum(self.delta, axis=1, keepdims=True)\n",
        "\n",
        "        l1_term = l1_lambda * np.sign(self.weights)\n",
        "\n",
        "        #update weights\n",
        "        self.m_w = beta1 * self.m_w + (1 - beta1) * dW\n",
        "        self.v_w = beta2 * self.v_w + (1 - beta2) * (dW ** 2)\n",
        "\n",
        "        m_w_hat = self.m_w / (1 - beta1 ** t)\n",
        "        v_w_hat = self.v_w / (1 - beta2 ** t)\n",
        "        self.weights -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon) + l1_term\n",
        "\n",
        "        #update bias\n",
        "        self.m_b = beta1 * self.m_b + (1 - beta1) * dB\n",
        "        self.v_b = beta2 * self.v_b + (1 - beta2) * (dB ** 2)\n",
        "\n",
        "        m_b_hat = self.m_b / (1 - beta1 ** t)\n",
        "        v_b_hat = self.v_b / (1 - beta2 ** t)\n",
        "        self.bias -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        self.hidden_layers = []\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        hidden_outputs = inputs\n",
        "        if len(self.hidden_layers) != 0:\n",
        "            for hidden_layer in self.hidden_layers:\n",
        "                hidden_outputs = hidden_layer.forward(hidden_outputs)\n",
        "        return hidden_outputs\n",
        "\n",
        "    def backward(self, y, output):\n",
        "        output_delta = binray_cross_entropy_derivative(y, output)\n",
        "        if len(self.hidden_layers) != 0:\n",
        "            self.hidden_backward(output_delta)\n",
        "\n",
        "    def update_weights(self, learning_rate, beta1, beta2, epsilon, t, l1_lambda = 0):\n",
        "\n",
        "        for layer in self.hidden_layers:\n",
        "            layer.update_weights(learning_rate, beta1, beta2, epsilon, t, l1_lambda)\n",
        "\n",
        "    def train(self, x, y, epochs = 7, learining_rate = 0.1, batch_size = 32, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, l1_lambda = 0):\n",
        "\n",
        "        num_batches = len(x) // batch_size\n",
        "\n",
        "        for i in range(epochs):\n",
        "            loss = 0\n",
        "            correct = 0\n",
        "\n",
        "            indices = np.random.permutation(len(x))\n",
        "            x_shuffled = x[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            for j in range(num_batches):\n",
        "\n",
        "                start_index = j * batch_size\n",
        "                end_index = start_index + batch_size\n",
        "                x_batch = x_shuffled[start_index:end_index]\n",
        "                y_batch = y_shuffled[start_index:end_index]\n",
        "\n",
        "                #forward pass\n",
        "                output = self.forward(x_batch.T)\n",
        "\n",
        "                loss += binray_cross_entropy(y_batch, output)\n",
        "                l1_loss = l1_lambda * self.calculate_l1_loss(l1_lambda)\n",
        "                loss += l1_loss\n",
        "                #calculating accuracy\n",
        "                predictions = np.where(output > 0.5, 1, 0)\n",
        "                correct += np.sum(predictions == y_batch)\n",
        "                self.backward(y_batch, output)\n",
        "\n",
        "                #update weights using adam\n",
        "                t = i *  num_batches + j + 1 # time step\n",
        "\n",
        "                #update weights\n",
        "                self.update_weights(learining_rate, beta1, beta2, epsilon, t, l1_lambda)\n",
        "\n",
        "            avr_loss = np.mean(loss) / num_batches\n",
        "            accuracy = correct / (num_batches * batch_size)\n",
        "            print(\"epoch {}, loss: {}, accuracy: {}\".format(i+1, avr_loss, accuracy))\n",
        "        return correct/len(x)\n",
        "\n",
        "    def test(self, x, y, epochs = 7, learining_rate = 0.1, batch_size = 32):\n",
        "\n",
        "        num_batches = len(x) // batch_size\n",
        "\n",
        "        for i in range(epochs):\n",
        "            loss = 0\n",
        "            correct = 0\n",
        "\n",
        "            indices = np.random.permutation(len(x))\n",
        "            x_shuffled = x[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            for j in range(num_batches):\n",
        "\n",
        "                start_index = j * batch_size\n",
        "                end_index = start_index + batch_size\n",
        "                x_batch = x_shuffled[start_index:end_index]\n",
        "                y_batch = y_shuffled[start_index:end_index]\n",
        "\n",
        "                #forward pass\n",
        "                output = self.forward(x_batch.T)\n",
        "\n",
        "                loss += binray_cross_entropy(y_batch, output)\n",
        "                #calculating accuracy\n",
        "                predictions = np.where(output > 0.5, 1, 0)\n",
        "                correct += np.sum(predictions == y_batch)\n",
        "\n",
        "                #update weights using adam\n",
        "                t = i *  num_batches + j + 1 # time step\n",
        "\n",
        "                #update weights\n",
        "\n",
        "            avr_loss = np.mean(loss) / num_batches\n",
        "            accuracy = correct / (num_batches * batch_size)\n",
        "            print(\"epoch {}, loss: {}, accuracy: {}\".format(i+1, avr_loss, accuracy))\n",
        "        return correct/len(x)\n",
        "\n",
        "    def predict(self, x):\n",
        "        output = self.forward(x)\n",
        "        return np.round(output)\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.hidden_layers.append(layer)\n",
        "\n",
        "    def hidden_backward(self, delta):\n",
        "        hidden_delta = delta\n",
        "        for hidden_layer in reversed(self.hidden_layers):\n",
        "            hidden_delta = hidden_layer.backward(hidden_delta)\n",
        "\n",
        "    def calculate_l1_loss(self, l1_lambda = 0):\n",
        "        l1_loss = 0\n",
        "        for layer in self.hidden_layers:\n",
        "            l1_loss += l1_lambda * np.sum(np.abs(layer.weights))\n",
        "        return l1_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVyp6GjCTTTW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "class Neuron:\n",
        "\n",
        "    def sigmoid(unactivated_output, derivative = False):\n",
        "        if derivative:\n",
        "            return Neuron.sigmoid(unactivated_output) * (1 - Neuron.sigmoid(unactivated_output))\n",
        "        activated_output = 1 / (1 + np.exp(-unactivated_output))\n",
        "        return activated_output\n",
        "\n",
        "\n",
        "    def relu(z, derivative = False, clip_threshod = 10000):\n",
        "        if derivative:\n",
        "            return np.where(z > 0, 1, 0)\n",
        "        return np.clip(z, 0, clip_threshod)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDMiG7EET34h",
        "outputId": "7872b5ef-c5d5-412e-e7dd-36f1ec140de8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "importing modules complete !\n",
            "reading the data ...\n",
            "reading data complete !\n",
            "splitting data ...\n",
            "splitting the data complete !\n",
            "defining functions ...\n",
            "defining functions complete !\n",
            "data processing ...\n",
            "data processing complete !\n",
            "converting to numbers ...\n",
            "converting to numbers complete !\n",
            "train test splitting ...\n",
            "splitting complete !\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd # to read data\n",
        "from keras.preprocessing.text import Tokenizer # for the dictionary\n",
        "from sklearn.model_selection import train_test_split # for splitting\n",
        "import pickle # to save files\n",
        "from nltk.stem import PorterStemmer # for stemming\n",
        "from nltk.tokenize import word_tokenize # to help stem\n",
        "from nltk.corpus import stopwords # to help stem\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english')) # to help stem\n",
        "import re # to help stem\n",
        "\n",
        "print(\"importing modules complete !\")\n",
        "\n",
        "print(\"reading the data ...\")\n",
        "path = '/content/drive/MyDrive/train.csv'\n",
        "df = pd.read_csv(path) # reading data\n",
        "\n",
        "print(\"reading data complete !\")\n",
        "\n",
        "print(\"splitting data ...\")\n",
        "x = df.iloc[:(int)(len(df.iloc[:,2])/17),2]\n",
        "y = df.iloc[:(int)(len(df.iloc[:,0])/17),0]\n",
        "del df\n",
        "print(\"splitting the data complete !\")\n",
        "inputLayer = 2250\n",
        "\n",
        "print(\"defining functions ...\")\n",
        "\n",
        "tokenizer = Tokenizer(num_words=inputLayer) # convert words to numbers\n",
        "def creating_dictionary(comments):\n",
        "    global tokenizer\n",
        "    tokenizer.fit_on_texts(comments)\n",
        "    data = tokenizer.texts_to_matrix(comments, mode='tfidf')\n",
        "    return data\n",
        "\n",
        "def convert_to_numbers(comments):\n",
        "    global tokenizer\n",
        "    data = tokenizer.texts_to_matrix(comments, mode='tfidf')\n",
        "    return data\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def data_processing(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text_tokens = word_tokenize(text)\n",
        "    filtered_text = [w for w in text_tokens if not w in stop_words]\n",
        "    stemmed_text = [stemmer.stem(word) for word in filtered_text]\n",
        "    return \" \".join(stemmed_text)\n",
        "\n",
        "print(\"defining functions complete !\")\n",
        "\n",
        "#applying data processing\n",
        "print(\"data processing ...\")\n",
        "#df.iloc[:,2] = df.iloc[:,2].apply(data_processing)\n",
        "x = x.apply(data_processing)\n",
        "\n",
        "print(\"data processing complete !\")\n",
        "\n",
        "y -= 1\n",
        "\n",
        "print(\"converting to numbers ...\")\n",
        "\n",
        "x = creating_dictionary(x)\n",
        "with open('/content/drive/MyDrive/tokenizer.pickle', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "del tokenizer\n",
        "print(\"converting to numbers complete !\")\n",
        "\n",
        "print(\"train test splitting ...\")\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.20,random_state =42)\n",
        "\n",
        "del x\n",
        "del y\n",
        "with open('/content/drive/MyDrive/x_train.pickle', 'wb') as f:\n",
        "    pickle.dump(x_train, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/y_train.pickle', 'wb') as f:\n",
        "    pickle.dump(y_train, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/x_test.pickle', 'wb') as f:\n",
        "    pickle.dump(x_test, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/y_test.pickle', 'wb') as f:\n",
        "    pickle.dump(y_test, f)\n",
        "\n",
        "print(\"splitting complete !\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "id": "Ln5veYhjbhxY",
        "outputId": "ce4f71d9-02eb-4b71-e889-7f6dbce730fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "importing modules ...\n",
            "Creating the model ...\n",
            "model complete !\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1, loss: 0.46910958606127773, accuracy: 0.8095544484321874\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-c19b912be295>:188: RuntimeWarning: overflow encountered in exp\n",
            "  activated_output = 1 / (1 + np.exp(-unactivated_output))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2, loss: 0.41370489676935107, accuracy: 0.8341990933131848\n",
            "epoch 3, loss: 0.4060119473514587, accuracy: 0.8214724216093691\n",
            "epoch 4, loss: 0.407618940563767, accuracy: 0.8134385625236117\n",
            "epoch 5, loss: 0.4077329186033673, accuracy: 0.8066147997733283\n",
            "epoch 6, loss: 0.4085867826845668, accuracy: 0.8033563940309785\n",
            "epoch 7, loss: 0.40524300499868143, accuracy: 0.8022938704193426\n",
            "epoch 8, loss: 0.404683788828714, accuracy: 0.8039821023800529\n",
            "epoch 9, loss: 0.4031753669597659, accuracy: 0.8026126275028334\n",
            "epoch 10, loss: 0.40306970053211144, accuracy: 0.8045251700037779\n",
            "0.8045109231395836\n",
            "model saved !\n",
            "like product\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c19b912be295>\u001b[0m in \u001b[0;36m<cell line: 287>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0mtest_data2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_numbers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneuraletwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0mprediction2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneuraletwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-c19b912be295>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhidden_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mhidden_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-c19b912be295>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (8,2250) and (1,2250) not aligned: 2250 (dim 1) != 1 (dim 0)"
          ]
        }
      ],
      "source": [
        "print(\"importing modules ...\")\n",
        "import numpy as np\n",
        "\n",
        "def binray_cross_entropy(y, p):\n",
        "    e = 1e-5\n",
        "    output = -(y * np.log(p + e) + (1 - y) * np.log(1 - p + e))\n",
        "    normalized_output = output\n",
        "    return normalized_output\n",
        "\n",
        "def binray_cross_entropy_derivative(y, p):\n",
        "    e = 1e-5\n",
        "    return (p - y ) / ((p + e) * (1 - p + e))\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, input_size, output_size, activation):\n",
        "        self.weights = np.random.randn(output_size,input_size)\n",
        "        self.bias = np.zeros((output_size,1))\n",
        "        self.activation = activation\n",
        "        self.inputs = None\n",
        "        self.outputs = None\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.m_w = np.zeros_like(self.weights)\n",
        "        self.v_w = np.zeros_like(self.weights)\n",
        "\n",
        "        self.m_b = np.zeros_like(self.bias)\n",
        "        self.v_b = np.zeros_like(self.bias)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        z = np.dot(self.weights, self.inputs) + self.bias\n",
        "        self.outputs = self.activation(z)\n",
        "        return self.outputs\n",
        "\n",
        "    def backward(self, delta):\n",
        "        activation_derivative = self.activation(self.outputs, derivative=True)\n",
        "        self.delta = delta * activation_derivative\n",
        "        input_delta = np.dot(self.weights.T, self.delta)\n",
        "        return input_delta\n",
        "\n",
        "    def update_weights(self, learning_rate, beta1, beta2, epsilon, t, l1_lambda = 0):\n",
        "        dW = np.dot(self.delta, self.inputs.T)\n",
        "        dB = np.sum(self.delta, axis=1, keepdims=True)\n",
        "\n",
        "        l1_term = l1_lambda * np.sign(self.weights)\n",
        "\n",
        "        #update weights\n",
        "        self.m_w = beta1 * self.m_w + (1 - beta1) * dW\n",
        "        self.v_w = beta2 * self.v_w + (1 - beta2) * (dW ** 2)\n",
        "\n",
        "        m_w_hat = self.m_w / (1 - beta1 ** t)\n",
        "        v_w_hat = self.v_w / (1 - beta2 ** t)\n",
        "        self.weights -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon) + l1_term\n",
        "\n",
        "        #update bias\n",
        "        self.m_b = beta1 * self.m_b + (1 - beta1) * dB\n",
        "        self.v_b = beta2 * self.v_b + (1 - beta2) * (dB ** 2)\n",
        "\n",
        "        m_b_hat = self.m_b / (1 - beta1 ** t)\n",
        "        v_b_hat = self.v_b / (1 - beta2 ** t)\n",
        "        self.bias -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        self.hidden_layers = []\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        hidden_outputs = inputs\n",
        "        if len(self.hidden_layers) != 0:\n",
        "            for hidden_layer in self.hidden_layers:\n",
        "                hidden_outputs = hidden_layer.forward(hidden_outputs)\n",
        "        return hidden_outputs\n",
        "\n",
        "    def backward(self, y, output):\n",
        "        output_delta = binray_cross_entropy_derivative(y, output)\n",
        "        if len(self.hidden_layers) != 0:\n",
        "            self.hidden_backward(output_delta)\n",
        "\n",
        "    def update_weights(self, learning_rate, beta1, beta2, epsilon, t, l1_lambda = 0):\n",
        "\n",
        "        for layer in self.hidden_layers:\n",
        "            layer.update_weights(learning_rate, beta1, beta2, epsilon, t, l1_lambda)\n",
        "\n",
        "    def train(self, x, y, epochs = 7, learining_rate = 0.1, batch_size = 32, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, l1_lambda = 0):\n",
        "\n",
        "        num_batches = len(x) // batch_size\n",
        "\n",
        "        for i in range(epochs):\n",
        "            loss = 0\n",
        "            correct = 0\n",
        "\n",
        "            indices = np.random.permutation(len(x))\n",
        "            x_shuffled = x[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            for j in range(num_batches):\n",
        "\n",
        "                start_index = j * batch_size\n",
        "                end_index = start_index + batch_size\n",
        "                x_batch = x_shuffled[start_index:end_index]\n",
        "                y_batch = y_shuffled[start_index:end_index]\n",
        "\n",
        "                #forward pass\n",
        "                output = self.forward(x_batch.T)\n",
        "\n",
        "                loss += binray_cross_entropy(y_batch, output)\n",
        "                l1_loss = l1_lambda * self.calculate_l1_loss(l1_lambda)\n",
        "                loss += l1_loss\n",
        "                #calculating accuracy\n",
        "                predictions = np.where(output > 0.5, 1, 0)\n",
        "                correct += np.sum(predictions == y_batch)\n",
        "                self.backward(y_batch, output)\n",
        "\n",
        "                #update weights using adam\n",
        "                t = i *  num_batches + j + 1 # time step\n",
        "\n",
        "                #update weights\n",
        "                self.update_weights(learining_rate, beta1, beta2, epsilon, t, l1_lambda)\n",
        "\n",
        "            avr_loss = np.mean(loss) / num_batches\n",
        "            accuracy = correct / (num_batches * batch_size)\n",
        "            print(\"epoch {}, loss: {}, accuracy: {}\".format(i+1, avr_loss, accuracy))\n",
        "        return correct/len(x)\n",
        "\n",
        "    def test(self, x, y, epochs = 7, learining_rate = 0.1, batch_size = 32):\n",
        "\n",
        "        num_batches = len(x) // batch_size\n",
        "\n",
        "        for i in range(epochs):\n",
        "            loss = 0\n",
        "            correct = 0\n",
        "\n",
        "            indices = np.random.permutation(len(x))\n",
        "            x_shuffled = x[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            for j in range(num_batches):\n",
        "\n",
        "                start_index = j * batch_size\n",
        "                end_index = start_index + batch_size\n",
        "                x_batch = x_shuffled[start_index:end_index]\n",
        "                y_batch = y_shuffled[start_index:end_index]\n",
        "\n",
        "                #forward pass\n",
        "                output = self.forward(x_batch.T)\n",
        "\n",
        "                loss += binray_cross_entropy(y_batch, output)\n",
        "                #calculating accuracy\n",
        "                predictions = np.where(output > 0.5, 1, 0)\n",
        "                correct += np.sum(predictions == y_batch)\n",
        "\n",
        "                #update weights using adam\n",
        "                t = i *  num_batches + j + 1 # time step\n",
        "\n",
        "                #update weights\n",
        "\n",
        "            avr_loss = np.mean(loss) / num_batches\n",
        "            accuracy = correct / (num_batches * batch_size)\n",
        "            print(\"epoch {}, loss: {}, accuracy: {}\".format(i+1, avr_loss, accuracy))\n",
        "        return correct/len(x)\n",
        "\n",
        "    def predict(self, x):\n",
        "        output = self.forward(x)\n",
        "        return np.round(output)\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.hidden_layers.append(layer)\n",
        "\n",
        "    def hidden_backward(self, delta):\n",
        "        hidden_delta = delta\n",
        "        for hidden_layer in reversed(self.hidden_layers):\n",
        "            hidden_delta = hidden_layer.backward(hidden_delta)\n",
        "\n",
        "    def calculate_l1_loss(self, l1_lambda = 0):\n",
        "        l1_loss = 0\n",
        "        for layer in self.hidden_layers:\n",
        "            l1_loss += l1_lambda * np.sum(np.abs(layer.weights))\n",
        "        return l1_loss\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "class Neuron:\n",
        "\n",
        "    def sigmoid(unactivated_output, derivative = False):\n",
        "        if derivative:\n",
        "            return Neuron.sigmoid(unactivated_output) * (1 - Neuron.sigmoid(unactivated_output))\n",
        "        activated_output = 1 / (1 + np.exp(-unactivated_output))\n",
        "        return activated_output\n",
        "\n",
        "\n",
        "    def relu(z, derivative = False, clip_threshod = 10000):\n",
        "        if derivative:\n",
        "            return np.where(z > 0, 1, 0)\n",
        "        return np.clip(z, 0, clip_threshod)\n",
        "\n",
        "inputLayer = 2250\n",
        "hidden = 8\n",
        "output = 1\n",
        "\n",
        "hidden_activation = Neuron.sigmoid\n",
        "input_activation = Neuron.sigmoid\n",
        "output_activation = Neuron.sigmoid\n",
        "\n",
        "neuraletwork = NeuralNetwork()\n",
        "\n",
        "l1 = Layer(inputLayer, hidden, hidden_activation)\n",
        "neuraletwork.add_layer(l1)\n",
        "\n",
        "l2 = Layer(hidden, hidden, hidden_activation)\n",
        "neuraletwork.add_layer(l2)\n",
        "\n",
        "l3 = Layer(hidden, hidden, hidden_activation)\n",
        "neuraletwork.add_layer(l3)\n",
        "\n",
        "l4 = Layer(hidden, hidden, hidden_activation)\n",
        "neuraletwork.add_layer(l4)\n",
        "\n",
        "l5 = Layer(hidden, hidden, hidden_activation)\n",
        "neuraletwork.add_layer(l5)\n",
        "\n",
        "output_layer = Layer(hidden, output, output_activation)\n",
        "neuraletwork.add_layer(output_layer)\n",
        "\n",
        "print(\"Creating the model ...\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"model complete !\")\n",
        "\n",
        "#y_train = y_train.values\n",
        "\n",
        "#x_train_normalized = (x_train - np.min(x_train)) / (np.max(x_train) - np.min(x_train))\n",
        "#x_test_normalized = (x_test - np.min(x_test)) / (np.max(x_test) - np.min(x_test))\n",
        "\n",
        "#del x_train\n",
        "#del x_test\n",
        "import pandas as pd # to read data\n",
        "from keras.preprocessing.text import Tokenizer # for the dictionary\n",
        "from sklearn.model_selection import train_test_split # for splitting\n",
        "import pickle # to save files\n",
        "from nltk.stem import PorterStemmer # for stemming\n",
        "from nltk.tokenize import word_tokenize # to help stem\n",
        "from nltk.corpus import stopwords # to help stem\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english')) # to help stem\n",
        "import re # to help stem\n",
        "def convert_to_numbers(comments):\n",
        "    with open('/content/drive/MyDrive/tokenizer.pickle', 'rb') as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "    data = tokenizer.texts_to_matrix(comments, mode='tfidf')\n",
        "    return data\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def data_processing(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text_tokens = word_tokenize(text)\n",
        "    filtered_text = [w for w in text_tokens if not w in stop_words]\n",
        "    stemmed_text = [stemmer.stem(word) for word in filtered_text]\n",
        "    return \" \".join(stemmed_text)\n",
        "\n",
        "with open('/content/drive/MyDrive/x_train.pickle', 'rb') as f:\n",
        "    x_train = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/y_train.pickle', 'rb') as f:\n",
        "    y_train = pickle.load(f)\n",
        "\n",
        "y_train = y_train.values\n",
        "accuracy = neuraletwork.train(x_train, y_train, epochs = 10,learining_rate = 0.1 / 2 ** 5, batch_size = 32, l1_lambda = 0.0001)\n",
        "print(accuracy)\n",
        "with open('/content/drive/MyDrive/Network.pickle', 'wb') as f:\n",
        "    pickle.dump(neuraletwork, f)\n",
        "print(\"model saved !\")\n",
        "text = 'i like this product'\n",
        "text2 = 'i hate and dislike this product'\n",
        "text = data_processing(text)\n",
        "text2 = data_processing(text2)\n",
        "\n",
        "test_data = convert_to_numbers([text])\n",
        "test_data2 = convert_to_numbers([text2])\n",
        "print(text)\n",
        "prediction = neuraletwork.forward(test_data)\n",
        "prediction2 = neuraletwork.forward(test_data2)\n",
        "print(\"prediction\",prediction)\n",
        "print(prediction2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "GJbtjcTExgBG",
        "outputId": "f99344a0-d7d4-4d9a-d157-e2913b44a020"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "importing modules ...\n",
            "Creating the model ...\n",
            "model complete !\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-073bf8412789>\u001b[0m in \u001b[0;36m<cell line: 236>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m \u001b[0;31m# to save files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
          ]
        }
      ],
      "source": [
        "print(\"importing modules ...\")\n",
        "import numpy as np\n",
        "\n",
        "def binray_cross_entropy(y, p):\n",
        "    e = 1e-5\n",
        "    output = -(y * np.log(p + e) + (1 - y) * np.log(1 - p + e))\n",
        "    normalized_output = output\n",
        "    return normalized_output\n",
        "\n",
        "def binray_cross_entropy_derivative(y, p):\n",
        "    e = 1e-5\n",
        "    return (p - y ) / ((p + e) * (1 - p + e))\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, input_size, output_size, activation):\n",
        "        self.weights = np.random.randn(output_size,input_size)\n",
        "        self.bias = np.zeros((output_size,1))\n",
        "        self.activation = activation\n",
        "        self.inputs = None\n",
        "        self.outputs = None\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.m_w = np.zeros_like(self.weights)\n",
        "        self.v_w = np.zeros_like(self.weights)\n",
        "\n",
        "        self.m_b = np.zeros_like(self.bias)\n",
        "        self.v_b = np.zeros_like(self.bias)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        z = np.dot(self.weights, self.inputs) + self.bias\n",
        "        self.outputs = self.activation(z)\n",
        "        return self.outputs\n",
        "\n",
        "    def backward(self, delta):\n",
        "        activation_derivative = self.activation(self.outputs, derivative=True)\n",
        "        self.delta = delta * activation_derivative\n",
        "        input_delta = np.dot(self.weights.T, self.delta)\n",
        "        return input_delta\n",
        "\n",
        "    def update_weights(self, learning_rate, beta1, beta2, epsilon, t, l1_lambda = 0):\n",
        "        dW = np.dot(self.delta, self.inputs.T)\n",
        "        dB = np.sum(self.delta, axis=1, keepdims=True)\n",
        "\n",
        "        l1_term = l1_lambda * np.sign(self.weights)\n",
        "\n",
        "        #update weights\n",
        "        self.m_w = beta1 * self.m_w + (1 - beta1) * dW\n",
        "        self.v_w = beta2 * self.v_w + (1 - beta2) * (dW ** 2)\n",
        "\n",
        "        m_w_hat = self.m_w / (1 - beta1 ** t)\n",
        "        v_w_hat = self.v_w / (1 - beta2 ** t)\n",
        "        self.weights -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon) + l1_term\n",
        "\n",
        "        #update bias\n",
        "        self.m_b = beta1 * self.m_b + (1 - beta1) * dB\n",
        "        self.v_b = beta2 * self.v_b + (1 - beta2) * (dB ** 2)\n",
        "\n",
        "        m_b_hat = self.m_b / (1 - beta1 ** t)\n",
        "        v_b_hat = self.v_b / (1 - beta2 ** t)\n",
        "        self.bias -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        self.hidden_layers = []\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        hidden_outputs = inputs\n",
        "        if len(self.hidden_layers) != 0:\n",
        "            for hidden_layer in self.hidden_layers:\n",
        "                hidden_outputs = hidden_layer.forward(hidden_outputs)\n",
        "        return hidden_outputs\n",
        "\n",
        "    def backward(self, y, output):\n",
        "        output_delta = binray_cross_entropy_derivative(y, output)\n",
        "        if len(self.hidden_layers) != 0:\n",
        "            self.hidden_backward(output_delta)\n",
        "\n",
        "    def update_weights(self, learning_rate, beta1, beta2, epsilon, t, l1_lambda = 0):\n",
        "\n",
        "        for layer in self.hidden_layers:\n",
        "            layer.update_weights(learning_rate, beta1, beta2, epsilon, t, l1_lambda)\n",
        "\n",
        "    def train(self, x, y, epochs = 7, learining_rate = 0.1, batch_size = 32, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, l1_lambda = 0):\n",
        "\n",
        "        num_batches = len(x) // batch_size\n",
        "\n",
        "        for i in range(epochs):\n",
        "            loss = 0\n",
        "            correct = 0\n",
        "\n",
        "            indices = np.random.permutation(len(x))\n",
        "            x_shuffled = x[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            for j in range(num_batches):\n",
        "\n",
        "                start_index = j * batch_size\n",
        "                end_index = start_index + batch_size\n",
        "                x_batch = x_shuffled[start_index:end_index]\n",
        "                y_batch = y_shuffled[start_index:end_index]\n",
        "\n",
        "                #forward pass\n",
        "                output = self.forward(x_batch.T)\n",
        "\n",
        "                loss += binray_cross_entropy(y_batch, output)\n",
        "                l1_loss = l1_lambda * self.calculate_l1_loss(l1_lambda)\n",
        "                loss += l1_loss\n",
        "                #calculating accuracy\n",
        "                predictions = np.where(output > 0.5, 1, 0)\n",
        "                correct += np.sum(predictions == y_batch)\n",
        "                self.backward(y_batch, output)\n",
        "\n",
        "                #update weights using adam\n",
        "                t = i *  num_batches + j + 1 # time step\n",
        "\n",
        "                #update weights\n",
        "                self.update_weights(learining_rate, beta1, beta2, epsilon, t, l1_lambda)\n",
        "\n",
        "            avr_loss = np.mean(loss) / num_batches\n",
        "            accuracy = correct / (num_batches * batch_size)\n",
        "            print(\"epoch {}, loss: {}, accuracy: {}\".format(i+1, avr_loss, accuracy))\n",
        "        return correct/len(x)\n",
        "\n",
        "    def test(self, x, y, epochs = 7, learining_rate = 0.1, batch_size = 32):\n",
        "\n",
        "        num_batches = len(x) // batch_size\n",
        "\n",
        "        for i in range(epochs):\n",
        "            loss = 0\n",
        "            correct = 0\n",
        "\n",
        "            indices = np.random.permutation(len(x))\n",
        "            x_shuffled = x[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            for j in range(num_batches):\n",
        "\n",
        "                start_index = j * batch_size\n",
        "                end_index = start_index + batch_size\n",
        "                x_batch = x_shuffled[start_index:end_index]\n",
        "                y_batch = y_shuffled[start_index:end_index]\n",
        "\n",
        "                #forward pass\n",
        "                output = self.forward(x_batch.T)\n",
        "\n",
        "                loss += binray_cross_entropy(y_batch, output)\n",
        "                #calculating accuracy\n",
        "                predictions = np.where(output > 0.5, 1, 0)\n",
        "                correct += np.sum(predictions == y_batch)\n",
        "\n",
        "                #update weights using adam\n",
        "                t = i *  num_batches + j + 1 # time step\n",
        "\n",
        "                #update weights\n",
        "\n",
        "            avr_loss = np.mean(loss) / num_batches\n",
        "            accuracy = correct / (num_batches * batch_size)\n",
        "            print(\"epoch {}, loss: {}, accuracy: {}\".format(i+1, avr_loss, accuracy))\n",
        "        return correct/len(x)\n",
        "\n",
        "    def predict(self, x):\n",
        "        output = self.forward(x)\n",
        "        return np.round(output)\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.hidden_layers.append(layer)\n",
        "\n",
        "    def hidden_backward(self, delta):\n",
        "        hidden_delta = delta\n",
        "        for hidden_layer in reversed(self.hidden_layers):\n",
        "            hidden_delta = hidden_layer.backward(hidden_delta)\n",
        "\n",
        "    def calculate_l1_loss(self, l1_lambda = 0):\n",
        "        l1_loss = 0\n",
        "        for layer in self.hidden_layers:\n",
        "            l1_loss += l1_lambda * np.sum(np.abs(layer.weights))\n",
        "        return l1_loss\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "class Neuron:\n",
        "\n",
        "    def sigmoid(unactivated_output, derivative = False):\n",
        "        if derivative:\n",
        "            return Neuron.sigmoid(unactivated_output) * (1 - Neuron.sigmoid(unactivated_output))\n",
        "        activated_output = 1 / (1 + np.exp(-unactivated_output))\n",
        "        return activated_output\n",
        "\n",
        "\n",
        "    def relu(z, derivative = False, clip_threshod = 10000):\n",
        "        if derivative:\n",
        "            return np.where(z > 0, 1, 0)\n",
        "        return np.clip(z, 0, clip_threshod)\n",
        "\n",
        "inputLayer = 2250\n",
        "hidden = 8\n",
        "output = 1\n",
        "\n",
        "hidden_activation = Neuron.sigmoid\n",
        "input_activation = Neuron.sigmoid\n",
        "output_activation = Neuron.sigmoid\n",
        "\n",
        "neuraletwork = NeuralNetwork()\n",
        "\n",
        "l1 = Layer(inputLayer, hidden, hidden_activation)\n",
        "neuraletwork.add_layer(l1)\n",
        "\n",
        "l2 = Layer(hidden, hidden, hidden_activation)\n",
        "neuraletwork.add_layer(l2)\n",
        "\n",
        "l3 = Layer(hidden, hidden, hidden_activation)\n",
        "neuraletwork.add_layer(l3)\n",
        "\n",
        "l4 = Layer(hidden, hidden, hidden_activation)\n",
        "neuraletwork.add_layer(l4)\n",
        "\n",
        "l5 = Layer(hidden, hidden, hidden_activation)\n",
        "neuraletwork.add_layer(l5)\n",
        "\n",
        "output_layer = Layer(hidden, output, output_activation)\n",
        "neuraletwork.add_layer(output_layer)\n",
        "\n",
        "print(\"Creating the model ...\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"model complete !\")\n",
        "\n",
        "import pandas as pd # to read data\n",
        "from keras.preprocessing.text import Tokenizer # for the dictionary\n",
        "from sklearn.model_selection import train_test_split # for splitting\n",
        "import pickle # to save files\n",
        "\n",
        "y_train = y_train.values\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "for i in range(3):\n",
        "  print(y_train[i])\n",
        "accuracy = neuraletwork.train(x_train, y_train, epochs = 10,learining_rate = 0.1 / 2 ** 7, batch_size = 32, l1_lambda = 0.0001)\n",
        "print(accuracy)\n",
        "with open('/content/drive/MyDrive/Network.pickle', 'wb') as f:\n",
        "    pickle.dump(neuraletwork, f)\n",
        "del x_train\n",
        "del x_test\n",
        "print(\"model saved !\")\n",
        "text = 'i like this product'\n",
        "text2 = 'i hate and dislike this product'\n",
        "text = data_processing(text)\n",
        "text2 = data_processing(text2)\n",
        "\n",
        "test_data = convert_to_numbers([text])\n",
        "test_data2 = convert_to_numbers([text2])\n",
        "print(text)\n",
        "print(test_data)\n",
        "print(test_data.shape)\n",
        "prediction = neuraletwork.forward(test_data[0])\n",
        "prediction2 = neuraletwork.forward(test_data2[0])\n",
        "print(\"prediction\",prediction)\n",
        "print(prediction2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "ve_zdmysXLam",
        "outputId": "14c254f5-f0fa-4e6c-ada4-be5f1c1c5d05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model complete !\n",
            "model saved !\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "like product\n",
            "hate dislik product bad doesnt\n",
            "1.623719051713635\n",
            "2.6291651551228643\n",
            "test data2 : \n",
            "2.6291651551228643\n",
            "2.868979919123254\n",
            "3.0545615126045145\n",
            "4.090499459983995\n",
            "5.896988226623647\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2d6ca76599ed>\u001b[0m in \u001b[0;36m<cell line: 243>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    241\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneuraletwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0mprediction2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneuraletwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-2d6ca76599ed>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhidden_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mhidden_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-2d6ca76599ed>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (8,2250) and (1,2250) not aligned: 2250 (dim 1) != 1 (dim 0)"
          ]
        }
      ],
      "source": [
        "print(\"model complete !\")\n",
        "\n",
        "import pandas as pd # to read data\n",
        "from keras.preprocessing.text import Tokenizer # for the dictionary\n",
        "from sklearn.model_selection import train_test_split # for splitting\n",
        "import pickle # to save files\n",
        "from nltk.stem import PorterStemmer # for stemming\n",
        "from nltk.tokenize import word_tokenize # to help stem\n",
        "from nltk.corpus import stopwords # to help stem\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english')) # to help stem\n",
        "import re # to help stem\n",
        "import numpy as np\n",
        "\n",
        "def binray_cross_entropy(y, p):\n",
        "    e = 1e-5\n",
        "    output = -(y * np.log(p + e) + (1 - y) * np.log(1 - p + e))\n",
        "    normalized_output = output\n",
        "    return normalized_output\n",
        "\n",
        "def binray_cross_entropy_derivative(y, p):\n",
        "    e = 1e-5\n",
        "    return (p - y ) / ((p + e) * (1 - p + e))\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, input_size, output_size, activation):\n",
        "        self.weights = np.random.randn(output_size,input_size)\n",
        "        self.bias = np.zeros((output_size,1))\n",
        "        self.activation = activation\n",
        "        self.inputs = None\n",
        "        self.outputs = None\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.m_w = np.zeros_like(self.weights)\n",
        "        self.v_w = np.zeros_like(self.weights)\n",
        "\n",
        "        self.m_b = np.zeros_like(self.bias)\n",
        "        self.v_b = np.zeros_like(self.bias)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        z = np.dot(self.weights, self.inputs) + self.bias\n",
        "        self.outputs = self.activation(z)\n",
        "        return self.outputs\n",
        "\n",
        "    def backward(self, delta):\n",
        "        activation_derivative = self.activation(self.outputs, derivative=True)\n",
        "        self.delta = delta * activation_derivative\n",
        "        input_delta = np.dot(self.weights.T, self.delta)\n",
        "        return input_delta\n",
        "\n",
        "    def update_weights(self, learning_rate, beta1, beta2, epsilon, t, l1_lambda = 0):\n",
        "        dW = np.dot(self.delta, self.inputs.T)\n",
        "        dB = np.sum(self.delta, axis=1, keepdims=True)\n",
        "\n",
        "        l1_term = l1_lambda * np.sign(self.weights)\n",
        "\n",
        "        #update weights\n",
        "        self.m_w = beta1 * self.m_w + (1 - beta1) * dW\n",
        "        self.v_w = beta2 * self.v_w + (1 - beta2) * (dW ** 2)\n",
        "\n",
        "        m_w_hat = self.m_w / (1 - beta1 ** t)\n",
        "        v_w_hat = self.v_w / (1 - beta2 ** t)\n",
        "        self.weights -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon) + l1_term\n",
        "\n",
        "        #update bias\n",
        "        self.m_b = beta1 * self.m_b + (1 - beta1) * dB\n",
        "        self.v_b = beta2 * self.v_b + (1 - beta2) * (dB ** 2)\n",
        "\n",
        "        m_b_hat = self.m_b / (1 - beta1 ** t)\n",
        "        v_b_hat = self.v_b / (1 - beta2 ** t)\n",
        "        self.bias -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        self.hidden_layers = []\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        hidden_outputs = inputs\n",
        "        if len(self.hidden_layers) != 0:\n",
        "            for hidden_layer in self.hidden_layers:\n",
        "                hidden_outputs = hidden_layer.forward(hidden_outputs)\n",
        "        return hidden_outputs\n",
        "\n",
        "    def backward(self, y, output):\n",
        "        output_delta = binray_cross_entropy_derivative(y, output)\n",
        "        if len(self.hidden_layers) != 0:\n",
        "            self.hidden_backward(output_delta)\n",
        "\n",
        "    def update_weights(self, learning_rate, beta1, beta2, epsilon, t, l1_lambda = 0):\n",
        "\n",
        "        for layer in self.hidden_layers:\n",
        "            layer.update_weights(learning_rate, beta1, beta2, epsilon, t, l1_lambda)\n",
        "\n",
        "    def train(self, x, y, epochs = 7, learining_rate = 0.1, batch_size = 32, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8, l1_lambda = 0):\n",
        "\n",
        "        num_batches = len(x) // batch_size\n",
        "\n",
        "        for i in range(epochs):\n",
        "            loss = 0\n",
        "            correct = 0\n",
        "\n",
        "            indices = np.random.permutation(len(x))\n",
        "            x_shuffled = x[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            for j in range(num_batches):\n",
        "\n",
        "                start_index = j * batch_size\n",
        "                end_index = start_index + batch_size\n",
        "                x_batch = x_shuffled[start_index:end_index]\n",
        "                y_batch = y_shuffled[start_index:end_index]\n",
        "\n",
        "                #forward pass\n",
        "                output = self.forward(x_batch.T)\n",
        "\n",
        "                loss += binray_cross_entropy(y_batch, output)\n",
        "                l1_loss = l1_lambda * self.calculate_l1_loss(l1_lambda)\n",
        "                loss += l1_loss\n",
        "                #calculating accuracy\n",
        "                predictions = np.where(output > 0.5, 1, 0)\n",
        "                correct += np.sum(predictions == y_batch)\n",
        "                self.backward(y_batch, output)\n",
        "\n",
        "                #update weights using adam\n",
        "                t = i *  num_batches + j + 1 # time step\n",
        "\n",
        "                #update weights\n",
        "                self.update_weights(learining_rate, beta1, beta2, epsilon, t, l1_lambda)\n",
        "\n",
        "            avr_loss = np.mean(loss) / num_batches\n",
        "            accuracy = correct / (num_batches * batch_size)\n",
        "            print(\"epoch {}, loss: {}, accuracy: {}\".format(i+1, avr_loss, accuracy))\n",
        "        return correct/len(x)\n",
        "\n",
        "    def test(self, x, y, epochs = 7, learining_rate = 0.1, batch_size = 32):\n",
        "\n",
        "        num_batches = len(x) // batch_size\n",
        "\n",
        "        for i in range(epochs):\n",
        "            loss = 0\n",
        "            correct = 0\n",
        "\n",
        "            indices = np.random.permutation(len(x))\n",
        "            x_shuffled = x[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            for j in range(num_batches):\n",
        "\n",
        "                start_index = j * batch_size\n",
        "                end_index = start_index + batch_size\n",
        "                x_batch = x_shuffled[start_index:end_index]\n",
        "                y_batch = y_shuffled[start_index:end_index]\n",
        "\n",
        "                #forward pass\n",
        "                output = self.forward(x_batch.T)\n",
        "\n",
        "                loss += binray_cross_entropy(y_batch, output)\n",
        "                #calculating accuracy\n",
        "                predictions = np.where(output > 0.5, 1, 0)\n",
        "                correct += np.sum(predictions == y_batch)\n",
        "\n",
        "                #update weights using adam\n",
        "                t = i *  num_batches + j + 1 # time step\n",
        "\n",
        "                #update weights\n",
        "\n",
        "            avr_loss = np.mean(loss) / num_batches\n",
        "            accuracy = correct / (num_batches * batch_size)\n",
        "            print(\"epoch {}, loss: {}, accuracy: {}\".format(i+1, avr_loss, accuracy))\n",
        "        return correct/len(x)\n",
        "\n",
        "    def predict(self, x):\n",
        "        output = self.forward(x)\n",
        "        return np.round(output)\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.hidden_layers.append(layer)\n",
        "\n",
        "    def hidden_backward(self, delta):\n",
        "        hidden_delta = delta\n",
        "        for hidden_layer in reversed(self.hidden_layers):\n",
        "            hidden_delta = hidden_layer.backward(hidden_delta)\n",
        "\n",
        "    def calculate_l1_loss(self, l1_lambda = 0):\n",
        "        l1_loss = 0\n",
        "        for layer in self.hidden_layers:\n",
        "            l1_loss += l1_lambda * np.sum(np.abs(layer.weights))\n",
        "        return l1_loss\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "class Neuron:\n",
        "\n",
        "    def sigmoid(unactivated_output, derivative = False):\n",
        "        if derivative:\n",
        "            return Neuron.sigmoid(unactivated_output) * (1 - Neuron.sigmoid(unactivated_output))\n",
        "        activated_output = 1 / (1 + np.exp(-unactivated_output))\n",
        "        return activated_output\n",
        "\n",
        "\n",
        "    def relu(z, derivative = False, clip_threshod = 10000):\n",
        "        if derivative:\n",
        "            return np.where(z > 0, 1, 0)\n",
        "        return np.clip(z, 0, clip_threshod)\n",
        "with open('/content/drive/MyDrive/Network.pickle', 'rb') as f:\n",
        "        neuraletwork = pickle.load(f)\n",
        "print(\"model saved !\")\n",
        "def convert_to_numbers(comments):\n",
        "    with open('/content/drive/MyDrive/tokenizer.pickle', 'rb') as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "    data = tokenizer.texts_to_matrix(comments, mode='tfidf')\n",
        "    return data\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def data_processing(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text_tokens = word_tokenize(text)\n",
        "    filtered_text = [w for w in text_tokens if not w in stop_words]\n",
        "    stemmed_text = [stemmer.stem(word) for word in filtered_text]\n",
        "    return \" \".join(stemmed_text)\n",
        "text = 'i like this product'\n",
        "text2 = \"i hate and dislike this product not bad doesn't\"\n",
        "text = data_processing(text)\n",
        "text2 = data_processing(text2)\n",
        "\n",
        "test_data = convert_to_numbers([text])\n",
        "test_data2 = convert_to_numbers([text2])\n",
        "print(text)\n",
        "print(text2)\n",
        "for i in test_data[0]:\n",
        "  if i != 0:\n",
        "    print(i)\n",
        "print(\"test data2 : \")\n",
        "for i in test_data2[0]:\n",
        "  if i != 0:\n",
        "    print(i)\n",
        "prediction = neuraletwork.forward(test_data)\n",
        "prediction2 = neuraletwork.forward(test_data2)\n",
        "print(\"prediction: \",prediction)\n",
        "print(\"prediction2: \",prediction2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}